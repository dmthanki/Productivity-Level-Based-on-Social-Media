# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VEAki2bsY3bBZhKXdPvWFTb_X06qFNKF
"""

# ========== Imports & Config ==========
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib
from joblib import dump
import streamlit as st

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import OneHotEncoder, StandardScaler,LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# ========== Step 1: Load ==========
DATA_PATH = "Dataset/social_media_vs_productivity.csv"
df = pd.read_csv(DATA_PATH)

print("Shape:", df.shape)
print(df.head())
print("\nDtypes:\n", df.dtypes)
print("\nMissing values per column:\n", df.isnull().sum())

# ========== Step 2: Create Target from actual_productivity_score ==========
assert 'actual_productivity_score' in df.columns, "Column 'actual_productivity_score' not found!"

def to_level(score):
    # Adjust these bins if your score scale differs
    if score >= 7:
        return "High"
    elif score >= 4:
        return "Medium"
    else:
        return "Low"

df['Productivity_Level'] = df['actual_productivity_score'].apply(to_level)
print("\nTarget distribution:\n", df['Productivity_Level'].value_counts())

# ========== Step 3: Quick EDA ==========
# Target distribution bar
counts = df['Productivity_Level'].value_counts()
plt.figure()
counts.plot(kind='bar')
plt.title('Class Distribution: Productivity_Level')
plt.xlabel('Class'); plt.ylabel('Count')
plt.show()

# A few numeric histograms (adjust list if you want others)
num_cols_quick = df.select_dtypes(include=np.number).columns.tolist()[:6]
for col in num_cols_quick:
    plt.figure()
    df[col].plot(kind='hist', bins=30)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col); plt.ylabel('Frequency')
    plt.show()

# ========== Step 4: Preprocessing & Split ==========
target_col = 'Productivity_Level'
# X = df.drop(columns=[target_col])
# y = df[target_col]

selected_features = ["age", "work_hours_per_day", "sleep_hours",
                     "perceived_productivity_score", "job_satisfaction_score"]  # or your chosen 5
X = df[selected_features]
y = df[target_col]

# Identify categorical & numerical columns
cat_cols = X.select_dtypes(include=['object']).columns.tolist()
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
print("Categorical columns:", cat_cols)
print("Numeric columns:", num_cols)

# OneHotEncoder: ensure compat across sklearn versions (sparse_output -> sparse)
ohe_kwargs = {}
try:
    # Newer sklearn
    ohe_kwargs = dict(handle_unknown='ignore', sparse_output=False)
    _ = OneHotEncoder(**ohe_kwargs)
except TypeError:
    # Older sklearn
    ohe_kwargs = dict(handle_unknown='ignore', sparse=False)

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(**ohe_kwargs))
])

preprocess = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, num_cols),
        ('cat', categorical_transformer, cat_cols)
    ]
)

# Stratified split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print("Train size:", X_train.shape, " Test size:", X_test.shape)

# ========== Step 5: Baseline Models ==========
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree":      DecisionTreeClassifier(random_state=42),
    "Random Forest":      RandomForestClassifier(random_state=42),
    "KNN":                KNeighborsClassifier(n_neighbors=5),
    "Naive Bayes":        GaussianNB(),
    "SVM (RBF)":          SVC(kernel='rbf', probability=True, random_state=42),
}

results = {}  # name -> (pipeline, acc, y_pred)

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)

for name, clf in models.items():
    pipe = Pipeline(steps=[('preprocess', preprocess),
                          ('model', clf)])
    pipe.fit(X_train, y_train_enc)
    y_pred = pipe.predict(X_test)
    acc = accuracy_score(y_test_enc, y_pred)

    y_pred_labels = le.inverse_transform(y_pred)

    results[name] = (pipe, acc, y_pred_labels)

    print(f"\n{name} Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred_labels))
    print("-" * 70)

print("\nSummary Accuracies:")
for k, v in results.items():
    print(f"{k}: {v[1]:.4f}")

# ========== Step 6: Confusion Matrices ==========
labels = sorted(y_test.unique().tolist())

for name, (pipe, acc, y_pred) in results.items():
    cm = confusion_matrix(y_test, y_pred, labels=labels)
    plt.figure()
    plt.imshow(cm, interpolation='nearest')
    plt.title(f'Confusion Matrix - {name} (Acc: {acc:.3f})')
    plt.colorbar()
    tick_marks = np.arange(len(labels))
    plt.xticks(tick_marks, labels, rotation=45)
    plt.yticks(tick_marks, labels)
    plt.xlabel('Predicted'); plt.ylabel('Actual')
    # numbers on boxes
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, cm[i, j], ha='center', va='center')
    plt.tight_layout()
    plt.show()

# ========== Step 7: Hyperparameter Tuning (RandomForest) ==========
rf = RandomForestClassifier(random_state=42)

param_dist = {
    'model__n_estimators':      [100, 200, 300],
    'model__max_depth':         [None, 10, 20, 30],
    'model__min_samples_split': [2, 5, 10],
    'model__min_samples_leaf':  [1, 2, 4]
}

rf_pipe = Pipeline(steps=[('preprocess', preprocess),
                         ('model', rf)])

random_search = RandomizedSearchCV(
    estimator=rf_pipe,
    param_distributions=param_dist,
    n_iter=20,      # keeps it fast
    cv=3,
    n_jobs=-1,
    verbose=2,
    random_state=42
)

random_search.fit(X_train, y_train)
print("\nBest Params:", random_search.best_params_)
best_rf_pipe = random_search.best_estimator_

y_pred_best = best_rf_pipe.predict(X_test)
print("\nTuned RandomForest Performance")
print("Accuracy:", accuracy_score(y_test, y_pred_best))
print(classification_report(y_test, y_pred_best))

cm = confusion_matrix(y_test, y_pred_best, labels=labels)
plt.figure()
plt.imshow(cm, interpolation='nearest')
plt.title('Confusion Matrix - Tuned RandomForest')
plt.colorbar()
tick_marks = np.arange(len(labels))
plt.xticks(tick_marks, labels, rotation=45)
plt.yticks(tick_marks, labels)
plt.xlabel('Predicted'); plt.ylabel('Actual')
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], ha='center', va='center')
plt.tight_layout()
plt.show()

# ========== Step 8: Feature Importance ==========
# Fit the preprocessor alone to extract final feature names after OHE
preprocess.fit(X_train)

rf_model = best_rf_pipe.named_steps['model']

# Case 1: If using numeric only
if 'num' in preprocess.named_transformers_:
    feature_names = preprocess.transformers_[0][2]

importances = rf_model.feature_importances_

fi = pd.DataFrame({'feature': feature_names, 'importance': importances}) \
       .sort_values('importance', ascending=False)

topn = 20
plt.figure()
plt.barh(fi.head(topn)['feature'][::-1], fi.head(topn)['importance'][::-1])
plt.title('Top Feature Importances (RandomForest)')
plt.xlabel('Importance'); plt.ylabel('Feature')
plt.tight_layout()
plt.show()

print("\nTop 30 features by importance:")
print(fi.head(30))

# ========== Step 9: Select Top-N Features ==========
top_features = fi.head(5)['feature'].tolist()   # <-- Change 5 to any N

print("\nSelected Top Features:", top_features)

# Reduce train/test to only top features
from sklearn.compose import ColumnTransformer

# Explicitly separate numerical/categorical for top features
num_features = ['actual_productivity_score', 'perceived_productivity_score',
                'job_satisfaction_score', 'work_hours_per_day', 'sleep_hours']

# Update preprocess with only these top features
preprocess_top = ColumnTransformer([
    ('num', StandardScaler(), [f for f in top_features if f in num_features])
    # ,('cat', OneHotEncoder(handle_unknown='ignore'), [f for f in top_features if f in cat_features])
])

# Example: retrain RF on top features
rf_pipe_top = Pipeline([
    ('preprocess', preprocess_top),
    ('model', RandomForestClassifier(random_state=42))
])

rf_pipe_top.fit(X_train[top_features], y_train)
y_pred_top = rf_pipe_top.predict(X_test[top_features])

print("\nAccuracy with Top Features:", accuracy_score(y_test, y_pred_top))

# ========== Step 9: Save Best Model ==========
MODEL_PATH = "best_random_forest_pipeline.joblib"
dump(best_rf_pipe, MODEL_PATH)
print("Saved model to:", MODEL_PATH)

best_model = results["Random Forest"][0]   # pipeline object
joblib.dump(best_model, "productivity_model.pkl")
joblib.dump(le, "label_encoder.pkl")

model = joblib.load("productivity_model.pkl")
le = joblib.load("label_encoder.pkl")

def predict_productivity(input_data: dict):
    """
    input_data = {
        "feature1": value,
        "feature2": value,
        ...
    }
    """
    df = pd.DataFrame([input_data])   # single-row DataFrame
    prediction = model.predict(df)
    label = le.inverse_transform(prediction)[0]
    return label

# Example usage
new_person = {
    "age": 29,
    "work_hours_per_day": 8,
    "sleep_hours": 6,
    "perceived_productivity_score": 7,
    "job_satisfaction_score": 8
}
print("Predicted Productivity:", predict_productivity(new_person))

st.title("Productivity Prediction")

age = st.number_input("Age", 18, 60, 25)
work_hours = st.slider("Work Hours Per Day", 0, 12, 8)
sleep = st.slider("Sleep Hours", 0, 12, 6)
perceived = st.slider("Perceived Productivity Score (1-10)", 1, 10, 7)
satisfaction = st.slider("Job Satisfaction Score (1-10)", 1, 10, 8)

if st.button("Predict"):
    data = {
        "age": age,
        "work_hours_per_day": work_hours,
        "sleep_hours": sleep,
        "perceived_productivity_score": perceived,
        "job_satisfaction_score": satisfaction
    }
    result = predict_productivity(data)
    st.success(f"Predicted Productivity: {result}")